{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import folium\n",
    "from matplotlib import pyplot as plt\n",
    "from multiprocessing import Pool"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:01:58.401834Z",
     "start_time": "2024-10-16T10:01:56.719205Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a8bf6d68200a29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Statistic for all-pair-paths between points of two clusters\n",
    "def get_statistic_per_point(graph: nx.Graph, c_from: int, c_to: int, _cls2hubs: dict[int:set[int]], log=False):\n",
    "    nodes_from = _cls2hubs[c_from]\n",
    "    nodes_to = _cls2hubs[c_to]\n",
    "    statistic = {}\n",
    "    iter = itertools.product(nodes_from, nodes_to)\n",
    "    generator = tqdm(iter, desc='find statistic', total=len(nodes_to) * len(nodes_from)) if log else iter\n",
    "    for f,t in generator:\n",
    "        p = nx.single_source_dijkstra(graph, f, t, weight='length')\n",
    "        cls = list(dict.fromkeys([int(graph.nodes()[u]['cluster']) for u in p[1]]))\n",
    "        cls = tuple(cls)\n",
    "        if cls not in statistic:\n",
    "            statistic[cls] = 0\n",
    "        statistic[cls] += 1\n",
    "    return statistic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:01:58.406987Z",
     "start_time": "2024-10-16T10:01:58.402950Z"
    }
   },
   "id": "6c9b2ee661caaab7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cluster to neighboring clusters\n",
    "def get_cls2n(graph: nx.Graph) -> dict[int: set[int]]:\n",
    "    _cls2n = {}\n",
    "    for u, du in graph.nodes(data=True):\n",
    "        for v in graph[u]:\n",
    "            dv = graph.nodes()[v]\n",
    "            if dv['cluster'] == du['cluster']:\n",
    "                continue\n",
    "            c1 = dv['cluster']\n",
    "            c2 = du['cluster']\n",
    "            if not (c1 in _cls2n):\n",
    "                _cls2n[c1] = set()\n",
    "            if not (c2 in _cls2n):\n",
    "                _cls2n[c2] = set()\n",
    "            _cls2n[c1].add(c2)\n",
    "            _cls2n[c2].add(c1)\n",
    "    return _cls2n\n",
    "\n",
    "\n",
    "# cluster then yts point that are connected with neighboring clusters\n",
    "def get_cls2hubs(graph: nx.Graph) -> dict[int: set[int]]:\n",
    "    _cls2hubs = {}\n",
    "    for u, du in graph.nodes(data=True):\n",
    "        for v in graph[u]:\n",
    "            dv = graph.nodes()[v]\n",
    "            c1 = du['cluster']\n",
    "            c2 = dv['cluster']\n",
    "            if c1 == c2:\n",
    "                continue\n",
    "            if not (c1 in _cls2hubs):\n",
    "                _cls2hubs[c1] = set()\n",
    "            if not (c2 in _cls2hubs):\n",
    "                _cls2hubs[c2] = set()\n",
    "            _cls2hubs[c1].add(u)\n",
    "            _cls2hubs[c2].add(v)\n",
    "    return _cls2hubs\n",
    "\n",
    "# функция для параллельного вычисления матрицы смежности\n",
    "def do_ad_mat(data):\n",
    "    g , points, cls2n , cls2hubs, worker = data.values()\n",
    "    ad_matrix={}\n",
    "    sleep(worker/10)\n",
    "    print(f'start: {worker}')\n",
    "    for i in tqdm(points, total=len(points), position=worker, desc=f'build ad_matrix, work:{worker}'):\n",
    "        ad_matrix[i] = {}\n",
    "        for j in set(cls2n[i]).union({i}):\n",
    "            data = get_statistic_per_point(g, i, j, cls2hubs, False)\n",
    "            cls0to1 = set()\n",
    "            for k, v in data.items():\n",
    "                for kk in k:\n",
    "                    cls0to1.add(kk)\n",
    "            ad_matrix[i][j] = cls0to1\n",
    "    return ad_matrix\n",
    "\n",
    "\n",
    "# adjacency matrix. it contains clusters that intersect with paths between two clusters\n",
    "def get_ad_matrix(\n",
    "        graph: nx.Graph,\n",
    "        communities: list[set[int]],\n",
    "        cls2hubs,\n",
    "        cls2n,\n",
    "        WORKERS = 4\n",
    ") -> dict[int: dict[int:set[int]]]:\n",
    "    ad_matrix = {}\n",
    "    \n",
    "    data = [{\n",
    "        'g': graph,\n",
    "        'points': [i for i in range(worker, len(communities), WORKERS)],\n",
    "        'cls2n': cls2n,\n",
    "        'cls2hubs': cls2hubs,\n",
    "        'worker': worker\n",
    "    } for worker in range(WORKERS)]\n",
    "    with Pool(WORKERS) as p:\n",
    "        res = p.map(do_ad_mat, data)\n",
    "    for r in res:\n",
    "        for i in r:\n",
    "            if i not in ad_matrix:\n",
    "                ad_matrix[i] = {}\n",
    "            for j in r[i]:\n",
    "                ad_matrix[i][j] = r[i][j]\n",
    "    return ad_matrix\n",
    "\n",
    "def get_r(_g:nx.Graph, u:int)->float:\n",
    "    nodes = len(_g.nodes())\n",
    "    summ_d = 0\n",
    "    paths = nx.shortest_path_length(_g, u, weight='length')\n",
    "    for _,l in paths.items():\n",
    "        summ_d+=l\n",
    "    return summ_d/nodes\n",
    "\n",
    "def get_min_dst(_g:nx.Graph, c1:int, c2:int)->float:\n",
    "    m = float('inf')\n",
    "    for u, du in _g.nodes(data=True):\n",
    "        if du['cluster']!=c1:\n",
    "            continue\n",
    "        for v,d in _g[u].items():\n",
    "            dv = _g.nodes()[v]\n",
    "            if dv['cluster']!=c2:\n",
    "                continue\n",
    "            l = d['length']\n",
    "            if l <m:\n",
    "                m=l\n",
    "    return m\n",
    "# build_center_graph\n",
    "def build_center_graph(\n",
    "        graph: nx.Graph,\n",
    "        communities: list[set[int]],\n",
    "        adjacency: dict[int:dict[int, set[int]]],\n",
    "        cls2n: dict[int: set[int]]\n",
    ") -> tuple[nx.Graph, dict[int, int]]:\n",
    "    x_graph = nx.Graph()\n",
    "    cls2c = {}\n",
    "    for cls, _ in tqdm(enumerate(communities), total=len(communities), desc='find centroids'):\n",
    "        gc = extract_cluster_list_subgraph(graph, [cls], communities)\n",
    "        min_node = nx.barycenter(gc, weight='length')[0]\n",
    "        du = graph.nodes()[min_node]\n",
    "        x_graph.add_node(graph.nodes()[min_node]['cluster'], **du)\n",
    "        cls2c[graph.nodes()[min_node]['cluster']] = min_node\n",
    "\n",
    "    if len(x_graph.nodes) == 1:\n",
    "        return x_graph, cls2c\n",
    "    \n",
    "    for u in tqdm(x_graph.nodes(), desc = 'find edges'):\n",
    "        for v in cls2n[u]:\n",
    "            g = extract_cluster_list_subgraph(graph, adjacency[u][v], communities)\n",
    "            l = nx.single_source_dijkstra(g, source=cls2c[u], target=cls2c[v], weight='length')[0]\n",
    "            # ll = get_min_dst(graph, u,v) \n",
    "            # ll_u = get_r(extract_cluster_list_subgraph(graph, [u], communities), cls2c[u])\n",
    "            # ll_v = get_r(extract_cluster_list_subgraph(graph, [v], communities), cls2c[v])\n",
    "            # print(l, ll, ll_u,ll_v, ll+ll_u+ll_v)\n",
    "            x_graph.add_edge(u, v, length=l)\n",
    "    return x_graph, cls2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:01:58.419729Z",
     "start_time": "2024-10-16T10:01:58.408925Z"
    }
   },
   "id": "236db12a822b1cb6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d2b696ee716410a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#load graph\n",
    "def get_graph(city_id: str = 'R2555133') -> nx.Graph:\n",
    "    gdf = ox.geocode_to_gdf(city_id, by_osmid=True)\n",
    "    polygon_boundary = gdf.unary_union\n",
    "    graph = ox.graph_from_polygon(polygon_boundary,\n",
    "                                  network_type='drive',\n",
    "                                  simplify=True)\n",
    "    G = nx.Graph(graph)\n",
    "    H = nx.Graph()\n",
    "    # Добавляем рёбра в новый граф, копируя только веса\n",
    "    for u, d in G.nodes(data=True):\n",
    "        H.add_node(u, x=d['x'], y=d['y'])\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        if u == v:\n",
    "            continue\n",
    "        da=H.nodes()[u]\n",
    "        db=H.nodes()[v]\n",
    "        # H.add_edge(u, v, length=((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000)\n",
    "        H.add_edge(u, v, length=d['length'])\n",
    "    del city_id, gdf, polygon_boundary, graph, G\n",
    "    return H\n",
    "\n",
    "#extract subgraph by clusters\n",
    "def extract_cluster_list_subgraph(graph: nx.Graph, cluster_number: list[int] | set[int], communities=None) -> nx.Graph:\n",
    "    if communities:\n",
    "        nodes_to_keep = [u for c in cluster_number for u in communities[c]]\n",
    "    else:\n",
    "        nodes_to_keep = [node for node, data in graph.nodes(data=True) if data['cluster'] in cluster_number]\n",
    "    return graph.subgraph(nodes_to_keep)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:01:59.260950Z",
     "start_time": "2024-10-16T10:01:59.255249Z"
    }
   },
   "id": "769b8cceb07d0805",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, BisectingKMeans\n",
    "\n",
    "\n",
    "#resolve_communities\n",
    "def resolve_communities(H: nx.Graph, **params) -> list[set[int]]:\n",
    "    communities = nx.community.louvain_communities(H,\n",
    "                                                   seed=1534,\n",
    "                                                   weight='length',\n",
    "                                                   resolution=params['r'])\n",
    "    cls = []\n",
    "    for i, c in enumerate(communities):\n",
    "        for n in nx.connected_components(H.subgraph(c)):\n",
    "            cls.append(n)\n",
    "    for i, ids in enumerate(cls):\n",
    "        for j in ids:\n",
    "            H.nodes()[j]['cluster'] = i\n",
    "    return cls\n",
    "\n",
    "def resolve_by_hdbscan(H: nx.Graph):\n",
    "    # paths = dict(tqdm(nx.all_pairs_dijkstra_path_length(H, weight='length'), total=(len(H.nodes()))))\n",
    "    def f(a,b):\n",
    "        u = int(a[2])\n",
    "        v = int(b[2])\n",
    "        # return paths[u][v]\n",
    "        if (u,v) in H.edges() or (v,u) in H.edges():\n",
    "            return H.edges()[(u,v)]['length']\n",
    "        return float('inf')\n",
    "        # return nx.single_source_dijkstra(g, u,v,weight='length')[0]\n",
    "    scan = HDBSCAN(metric=f, min_samples=1, max_cluster_size=30,n_jobs=-1,store_centers='medoid')\n",
    "    x = np.array([[d['x'], d['y'], u] for u, d in g.nodes(data=True)])\n",
    "    y = scan.fit_predict(x)\n",
    "    communities = {}\n",
    "    for i, u in enumerate(g.nodes):\n",
    "        cls = y[i]\n",
    "        if cls not in communities:\n",
    "            communities[cls] = set()\n",
    "        communities[cls].add(u)\n",
    "    communities = [communities[cls] for cls in communities]\n",
    "    cls = []\n",
    "    for i, c in enumerate(communities):\n",
    "        for n in nx.connected_components(g.subgraph(c)):\n",
    "            cls.append(n)\n",
    "    for i, ids in enumerate(cls):\n",
    "        for j in ids:\n",
    "            H.nodes()[j]['cluster'] = i\n",
    "    del scan\n",
    "    return cls"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:21:09.685740Z",
     "start_time": "2024-10-16T10:21:09.680299Z"
    }
   },
   "id": "3546eec44b2bc0ae",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(9496, 14786)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# примеры id есть в test_city.py\n",
    "g = get_graph('R71525')\n",
    "len(g.nodes), len(g.edges)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:02:18.486509Z",
     "start_time": "2024-10-16T10:02:12.067314Z"
    }
   },
   "id": "b219e78cf93171f5",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588676292 {'length': 21.732}\n",
      "12067885511 {'length': 177.201}\n",
      "12067885520 {'length': 29.774}\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T07:58:30.234023Z",
     "start_time": "2024-10-16T07:58:30.228679Z"
    }
   },
   "id": "458d55e381ef6570",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# оптимальное количество кластеров из статьи\n",
    "def get_opt_cluster_count(nodes:int)-> int:\n",
    "    alpha = 8.09 * (nodes ** (-0.48)) * (1 - 19.4 / (4.8 * np.log(nodes) + 8.8)) * nodes\n",
    "    return int(alpha) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e8b209b84edc412",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "88b1e8db57eccb6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "get_opt_cluster_count(len(g.nodes))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1053ecba87c88b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cms1 = resolve_communities(g, r=26)\n",
    "cms = resolve_by_hdbscan(g)\n",
    "print(len(cms))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:21:21.728517Z",
     "start_time": "2024-10-16T10:21:12.050823Z"
    }
   },
   "id": "23c1a5d32d570651",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc7f94a7b1dd0bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls2n = get_cls2n(g)\n",
    "cls2hubs = get_cls2hubs(g)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6594fe741a815f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "l = range(len(cms))\n",
    "iter = itertools.product(l,l)\n",
    "one = 0\n",
    "other = 0\n",
    "for n,(i,j) in tqdm(enumerate(iter), total = len(cms)**2):\n",
    "    stat = get_statistic_per_point(g, i, j, cls2hubs, False)\n",
    "    total = sum([x[1] for x in stat.items()])\n",
    "    stat = list(stat.items())\n",
    "    stat.sort(key=lambda x:-x[1])\n",
    "    stat=dict(stat)\n",
    "    if (len(stat)) == 1:\n",
    "        one+=1\n",
    "    else:\n",
    "        other+=1\n",
    "    if n > 200:\n",
    "        break\n",
    "    # print('stat:')\n",
    "    # for u,d in stat.items():\n",
    "    #     print(f'{d/total:.2f}')\n",
    "print(one, other)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f86331bf298d6faf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 182\n"
     ]
    }
   ],
   "source": [
    "print(one, other)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:07:20.897365Z",
     "start_time": "2024-10-16T10:07:20.894095Z"
    }
   },
   "id": "34a1cebee8acf432",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:0:   0%|          | 0/66 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b76375fa07584c369753f40ee5c68388"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:1:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b5596f643ba40fc80b245c10584941b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:2:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39473775004c461f89b481297ba330fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:3:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9957a7b96fac44b08259fccebb339b09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:4:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "645be58f12bb4b4ca06a78ac9416cabd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:5:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b18aedbcf14b4c278e03c0c7c247f46c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:6:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afc83f1aae2842a7a64342c02e3878fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:7:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9fdd9c0e2a3481d89e8dfe340afb86d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:8:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b8fc3d58e3e40099ea0c6117f805bf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:9:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca1610ee093145f7bebee882110142b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:10:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6c65b031cfc4762b8e2ced26473409a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:11:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec7b253937d140f5b9c3aac091a960f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:12:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "086f453a024248ebb4fa631f364c7d21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:13:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbefc7c86b2147fba18fd330d9e7188c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:14:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b69ad05fa154c99976c604447002f42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:15:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60f48a4486ba468a9dc7c08176301032"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:16:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2749de77c2ac4487a40b7d7c3988a6f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:17:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04bcf8dc5a56412c89cffc15e6e833d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:18:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2ea8a8eb8744b5483b2a2d66a2fa6a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": "build ad_matrix, work:19:   0%|          | 0/65 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caacfc6642dd44f1b7f51eb372913191"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ad_mat = get_ad_matrix(g, cms, cls2hubs, cls2n, WORKERS=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:15:26.318871Z",
     "start_time": "2024-10-16T10:07:34.007447Z"
    }
   },
   "id": "e2bc4ee81e611853",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T08:42:20.056376Z",
     "start_time": "2024-10-16T08:42:20.053307Z"
    }
   },
   "id": "109549702ee04b9d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "find centroids:   0%|          | 0/1301 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5f2aa99668d41a187589a5a66a13e96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "find edges:   0%|          | 0/1301 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30d6ae5761f5479ebbe5f1d768e3ded9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g1, cls2c = build_center_graph(g, cms, ad_mat, cls2n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:15:27.863855Z",
     "start_time": "2024-10-16T10:15:26.319843Z"
    }
   },
   "id": "718024858ba64653",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def build_geodetic(g_centers, ad_mat):\n",
    "    paths = dict(tqdm(nx.all_pairs_dijkstra_path(g_centers, weight='length'), total=(len(g_centers.nodes()))))\n",
    "    paths_len = dict(tqdm(nx.all_pairs_dijkstra_path_length(g_centers, weight='length'), total=(len(g_centers.nodes()))))\n",
    "    nodes = list(g_centers.nodes())\n",
    "    paths_t = [(nodes[i],nodes[j],paths_len[nodes[i]][nodes[j]],paths[nodes[i]][nodes[j]]) for i in range(len(nodes)) for j in range(i+1, len(nodes))]\n",
    "    del paths_len\n",
    "    del paths\n",
    "    paths_t.sort(key=lambda x:-len(x[3]))\n",
    "    data_graph = {u : {} for u in nodes }\n",
    "    for u in nodes:\n",
    "        for v in nodes:\n",
    "            data_graph[u][v]=None\n",
    "            \n",
    "    for u,v,length,path in tqdm(paths_t):\n",
    "        gg = None\n",
    "        num_nodes = 0\n",
    "        for i in range(len(path) - 1):\n",
    "            c1 = path[i]\n",
    "            c2 = path[i+1]\n",
    "            for cc in ad_mat[c1][c2]:\n",
    "                num_nodes += len(cms[cc])\n",
    "        cls = set()\n",
    "        for i in range(len(path) - 1):\n",
    "            u = path[i]\n",
    "            v = path[i + 1]\n",
    "            cu = g.nodes()[cls2c[u]]['cluster']\n",
    "            cv = g.nodes()[cls2c[v]]['cluster']\n",
    "            for c in ad_mat[cu][cv]:\n",
    "                cls.add(c)\n",
    "            cls.add(cu)\n",
    "            cls.add(cv)\n",
    "        new_num_nodes = sum([len(cms[c]) for c in cls])\n",
    "        for p1 in path:\n",
    "            for p2 in path:\n",
    "                gg0 = data_graph[p1][p2]\n",
    "                if gg0 is None:\n",
    "                    if gg is None:\n",
    "                        gg = extract_cluster_list_subgraph(g, path, cms)\n",
    "                    data_graph[p1][p2] = gg\n",
    "                    data_graph[p2][p1] = gg\n",
    "                    \n",
    "                    # data_graph[p1][p2] = cls\n",
    "                    # data_graph[p2][p1] = cls\n",
    "                    continue\n",
    "                # если новый кусок существенно меньше старого\n",
    "                # if num_nodes/10 > new_num_nodes:\n",
    "                #     if gg is None:\n",
    "                #         gg = extract_cluster_list_subgraph(g, path, cms)\n",
    "                #     data_graph[p1][p2] = gg\n",
    "                #     data_graph[p2][p1] = gg\n",
    "    return data_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:15:27.871244Z",
     "start_time": "2024-10-16T10:15:27.865405Z"
    }
   },
   "id": "f208a248aac0715",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# del data_graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T09:02:28.548743Z",
     "start_time": "2024-10-16T09:02:28.299503Z"
    }
   },
   "id": "7bd70c60d44e78d8",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1301 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "510787fd62764871b5436fc796197ee8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1301 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "521cbc66b2b2429998a84b96fa1f02f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/845650 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "487652d41d9f4557973993f5fdf96073"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_graph = build_geodetic(g1,ad_mat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.302689Z",
     "start_time": "2024-10-16T10:15:27.872890Z"
    }
   },
   "id": "8148498b6b4288c6",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# find path between two points\n",
    "def find_path_length_h(\n",
    "        g0: nx.Graph,\n",
    "        g1: nx.Graph,\n",
    "        adjacency: dict[int, dict[int:set[int]]],\n",
    "        cms: list[set[int]] | tuple[set[int]],\n",
    "        cls2c: dict[int:int],\n",
    "        from_node: int,\n",
    "        to_node: int,\n",
    "        cls2n: dict[int, set[int]]) -> float:\n",
    "    from_cluster = g0.nodes()[from_node]['cluster']\n",
    "    to_cluster = g0.nodes()[to_node]['cluster']\n",
    "\n",
    "    cls = {to_cluster, from_cluster}\n",
    "\n",
    "    def h(a, b):\n",
    "        da = g0.nodes()[cls2c[a]]\n",
    "        db = g0.nodes()[to_node]\n",
    "        return ((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000 * 0.56\n",
    "\n",
    "    # path = nx.single_source_dijkstra(g1, from_cluster, to_cluster, weight='length')[1]\n",
    "    path = nx.astar_path(g1, from_cluster, to_cluster, weight='length', heuristic=h)\n",
    "    for i in range(len(path) - 1):\n",
    "        u = path[i]\n",
    "        v = path[i + 1]\n",
    "        cu = g0.nodes()[cls2c[u]]['cluster']\n",
    "        cv = g0.nodes()[cls2c[v]]['cluster']\n",
    "        # for c in adjacency[cu][cv]:\n",
    "        #     cls.add(c)\n",
    "        cls.add(cu)\n",
    "        cls.add(cv)\n",
    "\n",
    "    # for c1 in cls2n[to_cluster].union({to_cluster}):\n",
    "    #     for c2 in cls2n[from_cluster].union({from_cluster}):\n",
    "    #         path = nx.astar_path(g1, c1, c2, h, weight='length')\n",
    "    #         for i in range(len(path) - 1):\n",
    "    #             u = path[i]\n",
    "    #             v = path[i + 1]\n",
    "    #             cu = g0.nodes()[cls2c[u]]['cluster']\n",
    "    #             cv = g0.nodes()[cls2c[v]]['cluster']\n",
    "    #             # for c in adjacency[cu][cv]:\n",
    "    #             #     cls.add(c)\n",
    "    #             cls.add(cu)\n",
    "    #             cls.add(cv)\n",
    "    def h(a, b):\n",
    "        da = g0.nodes()[a]\n",
    "        db = g0.nodes()[b]\n",
    "        return ((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000 * 0.56\n",
    "\n",
    "    g = extract_cluster_list_subgraph(g0, cls, cms)\n",
    "    # return nx.astar_path_length(g, from_node, to_node, weight='length', heuristic=h), []\n",
    "    return nx.single_source_dijkstra(g, from_node, to_node, weight='length')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.307719Z",
     "start_time": "2024-10-16T10:16:13.303960Z"
    }
   },
   "id": "4b342577bf04575e",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# find path between two points with few layer \n",
    "# метод не показал хороших реальтатов, ошибка растет примерно в 5 раз, ускорение не меняется\n",
    "def find_path_length_h_few_layer(\n",
    "        g0: nx.Graph,\n",
    "        g1: nx.Graph,\n",
    "        g2: nx.Graph,\n",
    "        cms: list[set[int]] | tuple[set[int]],\n",
    "        cms_1: list[set[int]] | tuple[set[int]],\n",
    "        cls2c: dict[int:int],\n",
    "        cls2c_1: dict[int:int],\n",
    "        from_node: int,\n",
    "        to_node: int,\n",
    "        ) -> float:\n",
    "    \n",
    "    from_cluster = g0.nodes()[from_node]['cluster']\n",
    "    to_cluster = g0.nodes()[to_node]['cluster']\n",
    "\n",
    "    from_cluster1 = g1.nodes()[from_cluster]['cluster']\n",
    "    to_cluster1 = g1.nodes()[to_cluster]['cluster']\n",
    "\n",
    "    cls = {to_cluster1, from_cluster1}\n",
    "    path = nx.single_source_dijkstra(g2, from_cluster1, to_cluster1, weight='length')[1]\n",
    "    \n",
    "    for i in range(len(path) - 1):\n",
    "        u = path[i]\n",
    "        v = path[i + 1]\n",
    "        cu = g1.nodes()[cls2c_1[u]]['cluster']\n",
    "        cv = g1.nodes()[cls2c_1[v]]['cluster']\n",
    "        cls.add(cu)\n",
    "        cls.add(cv)\n",
    "    g = extract_cluster_list_subgraph(g1, cls, cms_1)\n",
    "    \n",
    "    path = nx.single_source_dijkstra(g, from_cluster, to_cluster, weight='length')[1]\n",
    "    cls = {to_cluster, from_cluster}\n",
    "    for i in range(len(path) - 1):\n",
    "        u = path[i]\n",
    "        v = path[i + 1]\n",
    "        cu = g0.nodes()[cls2c[u]]['cluster']\n",
    "        cv = g0.nodes()[cls2c[v]]['cluster']\n",
    "        cls.add(cu)\n",
    "        cls.add(cv)\n",
    "\n",
    "    g = extract_cluster_list_subgraph(g0, cls, cms)\n",
    "    return nx.single_source_dijkstra(g, from_node, to_node, weight='length')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.331841Z",
     "start_time": "2024-10-16T10:16:13.308421Z"
    }
   },
   "id": "b5cc5db55eddc075",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# find path between two points\n",
    "def find_path_length_h_with_graphs(\n",
    "        g0: nx.Graph,\n",
    "        from_node: int,\n",
    "        to_node: int) -> float:\n",
    "    from_cluster = g0.nodes()[from_node]['cluster']\n",
    "    to_cluster = g0.nodes()[to_node]['cluster']\n",
    "    \n",
    "    def h(a, b):\n",
    "        da = g0.nodes()[a]\n",
    "        db = g0.nodes()[b]\n",
    "        return ((da['x'] - db['x']) ** 2 + (da['y'] - db ['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000 * 0.56\n",
    "\n",
    "    # return nx.astar_path_length(data_graph[from_cluster][to_cluster], from_node, to_node, weight='length', heuristic=h), []\n",
    "    \n",
    "    # g = extract_cluster_list_subgraph(g0,data_graph[from_cluster][to_cluster], cms)\n",
    "    g = data_graph[from_cluster][to_cluster]\n",
    "    return nx.single_source_dijkstra(g, from_node, to_node, weight='length')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.340626Z",
     "start_time": "2024-10-16T10:16:13.337734Z"
    }
   },
   "id": "3ad500622fb7b60e",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# пример второго слоя в кластеризации\n",
    "# from sklearn.cluster import KMeans, DBSCAN, HDBSCAN, BisectingKMeans\n",
    "# def f(a,b):\n",
    "#     u = int(a[2])\n",
    "#     v = int(b[2])\n",
    "#     if (u,v) in g1.edges() or (v,u) in g1.edges():\n",
    "#         return g1.edges()[(u,v)]['length']\n",
    "#     return float('inf')\n",
    "#     # return nx.single_source_dijkstra(g, u,v,weight='length')[0]\n",
    "# scan = HDBSCAN(metric=f, min_samples=1)\n",
    "# x = np.array([[d['x'], d['y'], u] for u, d in g1.nodes(data=True)])\n",
    "# y = scan.fit_predict(x)\n",
    "# communities = {}\n",
    "# for i, u in enumerate(g1.nodes):\n",
    "#     cls = y[i]\n",
    "#     if cls not in communities:\n",
    "#         communities[cls] = set()\n",
    "#     communities[cls].add(u)\n",
    "# communities = [communities[cls] for cls in communities]\n",
    "# cls = []\n",
    "# for i, c in enumerate(communities):\n",
    "#     for n in nx.connected_components(g1.subgraph(c)):\n",
    "#         cls.append(n)\n",
    "# for i, ids in enumerate(cls):\n",
    "#     for j in ids:\n",
    "#         g1.nodes()[j]['cluster'] = i\n",
    "# cms_1 = cls\n",
    "# len(cms_1)\n",
    "# cls2n_1 = get_cls2n(g1)\n",
    "# cls2hubs_1 = get_cls2hubs(g1)\n",
    "# ad_mat_1 = get_ad_matrix(g1, cms_1, cls2hubs_1, cls2n_1, WORKERS=20)\n",
    "# g2, cls2c_1 = build_center_graph(g1, cms_1, ad_mat_1, cls2n_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.336332Z",
     "start_time": "2024-10-16T10:16:13.332630Z"
    }
   },
   "id": "24ae55850197968",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "generate points:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a91abcd20524763916d5040a669960a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_node_for_initial_graph_v2(graph: nx.Graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    f, t = random.choice(nodes), random.choice(nodes)\n",
    "    while f == t:\n",
    "        f, t = random.choice(nodes), random.choice(nodes)\n",
    "    return f, t\n",
    "\n",
    "# if os.path.exists('../data/points.pkl'):\n",
    "#     with open('../data/points.pkl', 'rb') as fp:\n",
    "#         points = pickle.load(fp)\n",
    "#         fp.close()\n",
    "# else:\n",
    "points = [get_node_for_initial_graph_v2(g) for _ in trange(1000, desc='generate points')]\n",
    "# with open('../data/points.pkl', 'wb') as fp:\n",
    "#     pickle.dump(points, fp)\n",
    "#     fp.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.379808Z",
     "start_time": "2024-10-16T10:16:13.341059Z"
    }
   },
   "id": "950e8ced345d955d",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "check a-star\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d45ffe68be085"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.657349048030878"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_coef = 1\n",
    "def h_func(a, b):\n",
    "    da = g.nodes()[a]\n",
    "    db = g.nodes()[b]\n",
    "    return ((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000\n",
    "\n",
    "for e in g.edges():\n",
    "    u,v = e\n",
    "    du,dv=g.nodes()[u], g.nodes()[v]\n",
    "    w = g.edges()[e]['length']\n",
    "    h = h_func(u,v)\n",
    "    h_coef = min(h_coef, w/h)\n",
    "h_coef"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:13.410182Z",
     "start_time": "2024-10-16T10:16:13.380791Z"
    }
   },
   "id": "9b72452925fd24cc",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "h_coef_upper = 1\n",
    "def h_func(a, b):\n",
    "    da = g1.nodes()[a]\n",
    "    db = g1.nodes()[b]\n",
    "    return ((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000\n",
    "\n",
    "for e in g1.edges():\n",
    "    u,v = e\n",
    "    du,dv=g1.nodes()[u], g1.nodes()[v]\n",
    "    w = g1.edges()[e]['length']\n",
    "    h = h_func(u,v)\n",
    "    h_coef_upper = min(h_coef_upper, w/h)\n",
    "h_coef_upper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4a8360cd714c46c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def h(a, b):\n",
    "    da = g.nodes()[a]\n",
    "    db = g.nodes()[b]\n",
    "    return ((da['x'] - db['x']) ** 2 + (da['y'] - db['y']) ** 2) ** 0.5 / 360 * 2 * np.pi * 6371.01 * 1000 *h_coef\n",
    "\n",
    "def do_calc(data):\n",
    "    pps, i = data\n",
    "\n",
    "    stat = {}\n",
    "    stat['l'] = []\n",
    "    stat['h_l'] = []\n",
    "    stat['time_l'] = []\n",
    "    stat['time_h'] = []\n",
    "\n",
    "    stat['delta'] = []\n",
    "    sleep(i/10)\n",
    "    print('start', i)\n",
    "    for p1, p2 in tqdm(pps, desc='find paths', position=i):\n",
    "        if (p1, p2) in stat:\n",
    "            continue\n",
    "        num_iter = 2\n",
    "        length, p = None, None\n",
    "        start = time.time()\n",
    "        for _ in range(num_iter):\n",
    "            length, p = nx.single_source_dijkstra(g, p1, p2, weight='length')\n",
    "        time_l = time.time() - start\n",
    "        h_l, h_p = None, None\n",
    "        start = time.time()\n",
    "        for _ in range(num_iter):\n",
    "            h_l = nx.astar_path_length(g,  p1, p2, h,weight='length')\n",
    "        time_h = time.time() - start\n",
    "        delta = (h_l - length) / length * 100\n",
    "        stat['l'].append(length)\n",
    "        stat['h_l'].append(h_l)\n",
    "        stat['delta'].append(delta)\n",
    "        stat['time_l'].append(time_l)\n",
    "        stat['time_h'].append(time_h)\n",
    "\n",
    "    return stat\n",
    "astar = True\n",
    "if astar:\n",
    "    WORKER = 4\n",
    "    data = [([p for p in points[i::WORKER]], i) for i in range(WORKER)]\n",
    "    with Pool(WORKER) as p:\n",
    "        res = p.map(do_calc, data)\n",
    "    stat = {}\n",
    "    for l in res:\n",
    "        for d in l:\n",
    "            if d not in stat:\n",
    "                stat[d] = []\n",
    "            stat[d].extend(l[d])\n",
    "    print('err_mean:', np.mean(stat['delta']))\n",
    "    print('err_min:', np.min(stat['delta']))\n",
    "    print('err_max:', np.max(stat['delta']))\n",
    "    print(np.mean(np.array(stat['time_l']) / np.array(stat['time_h'])))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794616aaec2f6de5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "find paths:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "030793657245423ebf37fae653d8df1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "find paths:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59e7800d2e5947a58e207d19b5dd9a63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "find paths:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f9d572c3a114dfca772515281d2fdac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "find paths:   0%|          | 0/250 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da6ed1f2982f4c799be75ea679bc167d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err_mean: 10.959268867001466\n",
      "err_min: 0.0\n",
      "err_max: 1879.8556492308692\n",
      "19.088426832702645\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def do_calc(data):\n",
    "    pps, i = data\n",
    "\n",
    "    stat = {}\n",
    "    stat['l'] = []\n",
    "    stat['h_l'] = []\n",
    "    stat['p'] = []\n",
    "    stat['h_p'] = []\n",
    "    stat['time_l'] = []\n",
    "    stat['time_h'] = []\n",
    "\n",
    "    stat['delta'] = []\n",
    "    sleep(i/10)\n",
    "    print('start', i)\n",
    "    for p1, p2 in tqdm(pps, desc='find paths', position=i):\n",
    "        if (p1, p2) in stat:\n",
    "            continue\n",
    "        num_iter = 2\n",
    "        l, p = None, None\n",
    "        start = time.time()\n",
    "        for i in range(num_iter):\n",
    "            l, p = nx.single_source_dijkstra(g, p1, p2, weight='length')\n",
    "        time_l = time.time() - start\n",
    "        h_l, h_p = None, None\n",
    "        start = time.time()\n",
    "        for _ in range(num_iter):\n",
    "            # h_l, h_p  = find_path_length_k_paths(g, g1, ad_mat,cms, cls2c, p1, p2, cls2n)\n",
    "            # h_l, h_p = find_path_length_h(g, g1, ad_mat,cms, cls2c, p1, p2, cls2n)\n",
    "            # h_l, h_p = find_path_length_h(g, p1, p2)\n",
    "            h_l, h_p = find_path_length_h_with_graphs(g,p1, p2)\n",
    "        \n",
    "        time_h = time.time() - start\n",
    "        \n",
    "        delta = (h_l - l) / l * 100\n",
    "        stat['l'].append(l)\n",
    "        stat['h_l'].append(h_l)\n",
    "        stat['p'].append(p)\n",
    "        stat['h_p'].append(h_p)\n",
    "        stat['delta'].append(delta)\n",
    "        stat['time_l'].append(time_l)\n",
    "        stat['time_h'].append(time_h)\n",
    "\n",
    "    return stat\n",
    "\n",
    "WORKER = 4\n",
    "data = [([p for p in points[i::WORKER]], i) for i in range(WORKER)]\n",
    "# do_calc(data[0])\n",
    "with Pool(WORKER) as p:\n",
    "    res = p.map(do_calc, data)\n",
    "stat = {}\n",
    "for l in res:\n",
    "    for d in l:\n",
    "        if d not in stat:\n",
    "            stat[d] = []\n",
    "        stat[d].extend(l[d])\n",
    "\n",
    "print('err_mean:', np.mean(stat['delta']))\n",
    "print('err_min:', np.min(stat['delta']))\n",
    "print('err_max:', np.max(stat['delta']))\n",
    "print(np.mean(np.array(stat['time_l']) / np.array(stat['time_h'])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T10:16:22.346779Z",
     "start_time": "2024-10-16T10:16:13.410711Z"
    }
   },
   "id": "d7674e5d2cbc4bb2",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = stat['delta']\n",
    "data = np.array(data)\n",
    "# data = data[data<10]\n",
    "x = data\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.set(xlabel='err, %', ylabel='count')\n",
    "# ax[0].hist(x,bins=40, density=True, color='grey')\n",
    "\n",
    "hist, bins = np.histogram(x, bins=100)\n",
    "ax.bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1] - bins[0]))\n",
    "# ax.set_   \n",
    "# print(bins)\n",
    "# ax[0].set_title('normed=True')\n",
    "ax.set_title('hist = hist / hist.sum()')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3741ad6279790a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.DataFrame.from_dict(stat)\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(20)\n",
    "\n",
    "df.hist(column=['delta'], density=1, bins=10, ax=axs, xlabelsize=20, ylabelsize=20)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbaff5b7821ee461",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24171ff672d5637d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "f = df.get(['delta', 'l', 'h_l'])\n",
    "# plt.scatter(f['l'], f['delta'])\n",
    "# plt.scatter(f['h_l'], f['delta'])\n",
    "\n",
    "x = np.linspace(min(f['l']), max(f['l']), 100)\n",
    "plt.plot(x, x, c='red')\n",
    "plt.scatter(f['l'], f['h_l'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b29c28d558feb5da",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "delta = f['h_l'] - f['l']\n",
    "print(f\"\"\"\n",
    "max: {np.max(delta):.2f}\n",
    "mean: {np.mean(delta):.2f} +- {np.std(delta):.2f}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84837cdae3f447b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(f['l'], f['h_l'])\n",
    "# coefficient_of_dermination = r2_score(f['l'], f['h_l'])\n",
    "print(f\"\"\"\n",
    "r: {r_value}\n",
    "p: {p_value}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d60f31ae922be5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def line(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "popt = curve_fit(line, f['l'], f['h_l'])\n",
    "print(f\"\"\"\n",
    "a: {popt[0][0]:.4f} +- {np.sqrt(popt[1][0, 0]):.8f}\n",
    "b: {popt[0][1]:.4f} +- {np.sqrt(popt[1][1, 1]):.8f}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cd84b69640e94ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b94e38be2b00e026",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_color_list(l: int):\n",
    "    cmap = plt.get_cmap('plasma')\n",
    "    colors = [cmap(i / l) for i in range(l)]\n",
    "    hex_colors = ['#' + ''.join([f'{int(c * 255):02x}' for c in color[:3]]) \\\n",
    "                  for color in colors]\n",
    "    return hex_colors\n",
    "\n",
    "\n",
    "def draw_on_map(_g: nx.Graph,\n",
    "                communities: tuple = None,\n",
    "                m: folium.Map = None,\n",
    "                node_colors: list | str = None,\n",
    "                edge_colors: str = 'black') -> folium.Map:\n",
    "    if communities is None:\n",
    "        communities = [{u} for u in _g.nodes()]\n",
    "    if node_colors is None:\n",
    "        node_colors = get_color_list(len(communities))\n",
    "\n",
    "    if m is None:\n",
    "        for u, d in _g.nodes(data=True):\n",
    "            u_x, u_y = d['x'], d['y']\n",
    "            break\n",
    "        m = folium.Map(location=[u_y, u_x], zoom_start=10)  # Координаты города\n",
    "    for i, community in enumerate(communities):\n",
    "            \n",
    "        for node in community:\n",
    "            if node not in _g.nodes():\n",
    "                continue\n",
    "            node_data = _g.nodes[node]\n",
    "            popup_text = f\"Кластер: {i}, \\n\" + f\"номер: {node}\"\n",
    "            folium.CircleMarker(\n",
    "                location=(node_data['y'], node_data['x']),\n",
    "                radius=4,\n",
    "                color=node_colors[i] if isinstance(node_colors, list) else node_colors,\n",
    "                fill=True,\n",
    "                fill_color=node_colors[i] if isinstance(node_colors, list) is list else node_colors,\n",
    "                fill_opacity=0.7,\n",
    "                popup=popup_text\n",
    "            ).add_to(m)\n",
    "    if not (edge_colors is None):\n",
    "        for u, v, data in _g.edges(data=True):\n",
    "            u_x, u_y = _g.nodes()[u]['x'], _g.nodes()[u]['y']\n",
    "            v_x, v_y = _g.nodes()[v]['x'], _g.nodes()[v]['y']\n",
    "            folium.PolyLine([(u_y, u_x), (v_y, v_x)], color=edge_colors, weight=1).add_to(m)\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a66e853cda3113e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# рисовалка для osm\n",
    "# m = draw_on_map(g, node_colors='red')\n",
    "# m.save('msk_without_2_deg.html')\n",
    "# m.show_in_browser()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2053e67c42b098b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# рисует топ по ошибкам\n",
    "sorted = np.argsort(stat['delta'])\n",
    "index = -1\n",
    "while stat['delta'][sorted[index]] > 5:\n",
    "    print(stat['delta'][sorted[index]])\n",
    "    max_index = np.argsort(stat['delta'])[index]\n",
    "    index -= 1\n",
    "    correct_path = g.subgraph(stat['p'][max_index])\n",
    "    h_path = g.subgraph(stat['h_p'][max_index])\n",
    "    # continue\n",
    "    clusters = set()\n",
    "    for i in range(len(correct_path) - 1):\n",
    "        c1 = g.nodes()[stat['p'][max_index][i]]['cluster']\n",
    "        c2 = g.nodes()[stat['p'][max_index][i + 1]]['cluster']\n",
    "        clusters.add(c1)\n",
    "        if c1 != c2:\n",
    "            clusters = clusters.union(ad_mat[c1][c2])\n",
    "    for i in range(len(h_path) - 1):\n",
    "        c1 = g.nodes()[stat['h_p'][max_index][i]]['cluster']\n",
    "        c2 = g.nodes()[stat['h_p'][max_index][i + 1]]['cluster']\n",
    "        clusters.add(c1)\n",
    "        if c1 != c2:\n",
    "            clusters = clusters.union(ad_mat[c1][c2])\n",
    "\n",
    "    sub_graph = nx.Graph(extract_cluster_list_subgraph(g, clusters, cms))\n",
    "    cls = {}\n",
    "    counter = 0\n",
    "    for c in clusters:\n",
    "        cls[c] = counter\n",
    "        counter += 1\n",
    "    for u, d in sub_graph.nodes(data=True):\n",
    "        d['cluster'] = cls[d['cluster']]\n",
    "\n",
    "    pos_sub_graph = {u: (d['x'], d['y']) for u, d in sub_graph.nodes(data=True)}\n",
    "    pos_correct_path = {u: (d['x'], d['y']) for u, d in correct_path.nodes(data=True)}\n",
    "    pos_h_path = {u: (d['x'], d['y']) for u, d in h_path.nodes(data=True)}\n",
    "    pos_centers = {u: (d['x'], d['y']) for u, d in g1.nodes(data=True)}\n",
    "    # pos_G = {u: (d['x'], d['y']) for u, d in G.nodes(data=True)}\n",
    "\n",
    "    cmap = plt.get_cmap('plasma')\n",
    "    colors_sub_graph = [cmap(d['cluster'] / len(clusters)) for u, d in sub_graph.nodes(data=True)]\n",
    "    colors_sub_centers = [cmap(d['cluster'] / len(clusters)) for u, d in g1.nodes(data=True)]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(10)\n",
    "    e = stat['delta'][sorted[index + 1]]\n",
    "    axs.set_title(f'err: {e:.2f}')\n",
    "    label_dict = {u: d['cluster'] for u, d in sub_graph.nodes(data=True)}\n",
    "\n",
    "    nx.draw(sub_graph, ax=axs, node_size=50, pos=pos_sub_graph, alpha=0.5, node_color=colors_sub_graph)\n",
    "    # nx.draw(g1, ax=axs, node_size=50, pos=pos_centers, node_color='white')\n",
    "    nx.draw(correct_path, ax=axs, node_size=50, pos=pos_correct_path, node_color='red')\n",
    "    nx.draw(h_path, ax=axs, node_size=50, pos=pos_h_path, node_color='green')\n",
    "    pos_sub_graph = {u: (x + 0.0005, y) for u, (x, y) in pos_sub_graph.items()}\n",
    "    # nx.draw_networkx_labels(sub_graph, pos_sub_graph, labels=label_dict, font_size=15)\n",
    "    # nx.draw(H, ax=axs,node_size = 50, pos=pos_H, node_color = 'red', edge_color = 'green')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c9199bd6441396",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
